{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Huggingface pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the pipelines is a very simple way to use pretrained models for **inference**. \n",
    "\n",
    "**Pipelines** can be used in a very simple way for applying **existing models (available in huggingface)** to your own data.\n",
    "\n",
    "The models that can be used in pipelines deal with both **uni-modal** and **multi-modal** tasks.\n",
    "\n",
    "## General usage:\n",
    "\n",
    "A pipeline is a **wrapper** built for working around all the other available pipelines. \n",
    "\n",
    "It can be executed on:\n",
    "\n",
    "- a single item\n",
    "- a list of items (list)\n",
    "- a Dataset object\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "import logging, sys\n",
    "logging.disable(sys.maxsize)\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "from IPython.core.display import HTML\n",
    "\n",
    "def colored(s, color='blue'):\n",
    "    return \"<text style=color:{}>{}</text>\".format(color, s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature extraction from text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### on a single item\n",
    "\n",
    "The result is a **tensor** of shape [1, sequence_lenth, hidden_dimension] representing the input string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['this', 'is', 'a', 'short', 'sentence', '.']\n",
      "torch.Size([1, 8, 768])\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "import torch\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "\n",
    "model = \"bert-base-uncased\"\n",
    "task = \"feature-extraction\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "\n",
    "\n",
    "text = \"This is a short sentence.\"\n",
    "tokenized_text = tokenizer.tokenize(text)\n",
    "\n",
    "\n",
    "feature_extractor = pipeline(task = task, model=model, tokenizer=tokenizer)\n",
    "result = feature_extractor(text, return_tensors=True)\n",
    "\n",
    "print(tokenized_text)\n",
    "print(result.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### on a list of items (i.e., sentences)\n",
    "#### The result is list of tensors\n",
    "- each tensor of shape [1, sequence_lenth, hidden_dimension] represents one input string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 12, 768])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = [\"an suv sitting on top of a cross walk.\",\n",
    "             \"the suv is stopped in the middle of the crosswalkj.\",\n",
    "             \"there are people crossing the street and a car in the cross walk\",\n",
    "             \"an intersection at night and the light is red.\",\n",
    "             \"the car stops on the crosswalk to allow pedestrians to cross the street safely.\"]\n",
    "feature_extractor = pipeline(model=\"bert-base-uncased\", task=\"feature-extraction\")\n",
    "result_list = feature_extractor(sentences, return_tensors=True)\n",
    "\n",
    "print(len(result_list))\n",
    "result_list[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-defined pipelines are also useful for inference on the most important NLP, Vision and Multi-modal tasks.\n",
    "\n",
    "### The Focus of this notebook:\n",
    "\n",
    "#### Vision tasks:\n",
    "\n",
    "- Object Detection\n",
    "- Image Classification\n",
    "- Zero-shot Object Detection\n",
    "\n",
    "#### Multi-modal tasks:\n",
    "- Automatic Captioning\n",
    "- Document Question Answering \n",
    "- Visual Question Answering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vision tasks:\n",
    "\n",
    "### Object Detection Pipeline:\n",
    "\n",
    "This pipeline can be used with any AutoModelForObjectDetection. It predicts bounding boxes of objects and their classes.\n",
    "\n",
    "#### Inferring from [**facebook/detr-resnet-50**](https://huggingface.co/facebook/detr-resnet-50)\n",
    "\n",
    "The DETR model is an encoder-decoder transformer with a convolutional backbone. Two heads are added on top of the decoder outputs in order to perform object detection: a linear layer for the class labels and a MLP (multi-layer perceptron) for the bounding boxes. The model uses object queries to detect objects in an image. Each object query looks for a particular object in the image.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://farm4.staticflickr.com/3236/2474976343_15aabea22b_z.jpg\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'score': 0.9331260323524475,\n",
       "  'label': 'sandwich',\n",
       "  'box': {'xmin': 24, 'ymin': 209, 'xmax': 290, 'ymax': 377}},\n",
       " {'score': 0.9893137812614441,\n",
       "  'label': 'hot dog',\n",
       "  'box': {'xmin': 197, 'ymin': 16, 'xmax': 414, 'ymax': 152}},\n",
       " {'score': 0.9176713228225708,\n",
       "  'label': 'dining table',\n",
       "  'box': {'xmin': 0, 'ymin': 4, 'xmax': 639, 'ymax': 474}},\n",
       " {'score': 0.9770232439041138,\n",
       "  'label': 'chair',\n",
       "  'box': {'xmin': 0, 'ymin': 1, 'xmax': 267, 'ymax': 474}}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "from IPython.display import Image, display\n",
    "\n",
    "\n",
    "image_url = \"https://farm4.staticflickr.com/3236/2474976343_15aabea22b_z.jpg\"\n",
    "detector = pipeline(model=\"facebook/detr-resnet-50\")\n",
    "\n",
    "display(Image(url= image_url))\n",
    "detector(image_url)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's try with a different (smaller) model\n",
    "\n",
    "We use [**hustvl/yolos-tiny**](https://huggingface.co/hustvl/yolos-tiny) for inference.\n",
    "\n",
    "The YOLOS model is a Vision Transformer (ViT) trained using the DETR loss. Despite its simplicity, a base-sized YOLOS model is able to achieve 42 AP on COCO validation 2017 (similar to DETR and more complex frameworks such as Faster R-CNN).\n",
    "\n",
    "It was pre-trained on ImageNet-1k and fine-tuned on COCO 2017 object detection (300 epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://farm4.staticflickr.com/3236/2474976343_15aabea22b_z.jpg\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'score': 0.9307621121406555,\n",
       "  'label': 'dining table',\n",
       "  'box': {'xmin': 0, 'ymin': 2, 'xmax': 640, 'ymax': 473}}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "from IPython.display import Image, display\n",
    "import timm\n",
    "\n",
    "image_url = \"https://farm4.staticflickr.com/3236/2474976343_15aabea22b_z.jpg\"\n",
    "detector = pipeline(model=\"hustvl/yolos-tiny\")\n",
    "\n",
    "display(Image(url= image_url))\n",
    "detector(image_url)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vision tasks:\n",
    "\n",
    "### Image Classification Pipeline:\n",
    "\n",
    "The pipeline can be used with any AutoModelForImageClassification [such as](https://huggingface.co/models?filter=image-classification). It predicts the class of an image.\n",
    "\n",
    " \n",
    "#### Inferring from [**beit-base-patch16-224-pt22k-ft22k**](https://huggingface.co/microsoft/beit-base-patch16-224-pt22k-ft22k)\n",
    "\n",
    "The BEiT model is a Vision Transformer (ViT), which is a transformer encoder model (BERT-like). It is pretrained on a large collection of images (ImageNet21k) in a self-supervised fashion and then fine-tuned in a supervised fashion on ImageNet (1 million images and 1,000 classes).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"http://images.cocodataset.org/val2017/000000039769.jpg\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'score': 0.20681101083755493, 'label': 'tabby, tabby_cat'},\n",
       " {'score': 0.09384264796972275, 'label': 'tabby, queen'},\n",
       " {'score': 0.05316951125860214, 'label': 'kitten, kitty'},\n",
       " {'score': 0.04978814721107483, 'label': 'beanbag'},\n",
       " {'score': 0.03999922797083855, 'label': 'reliquary'}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "image_url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n",
    "\n",
    "classifier = pipeline(model=\"microsoft/beit-base-patch16-224-pt22k-ft22k\")\n",
    "\n",
    "display(Image(url= image_url))\n",
    "classifier(image_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vision tasks:\n",
    "\n",
    "### Zero-Shot Object Detection Pipeline:\n",
    "\n",
    "This pipeline is based on OwlViTForObjectDetection. It predicts bounding boxes of objects (requires a set of candidate_labels).\n",
    "\n",
    "#### Inferring from [**google/owlvit-base-patch32**](https://huggingface.co/google/owlvit-base-patch32)\n",
    "\n",
    "The model uses a CLIP backbone with a ViT-B/32 Transformer architecture as an image encoder and uses a masked self-attention Transformer as a text encoder. These encoders are trained to maximize the similarity of (image, text) pairs via a contrastive loss. \n",
    "\n",
    "The CLIP backbone is trained from scratch and fine-tuned together with the box and class prediction heads with an object detection objective.\n",
    "\n",
    "The Pipeline can be used also with [other models](https://huggingface.co/models?other=zero-shot-object-detection)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://farm4.staticflickr.com/3266/3247626615_b3ab8a85af_z.jpg\" width=\"500\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detections: man (score:0.4218994379043579\n",
      "{'xmin': 341, 'ymin': 105, 'xmax': 467, 'ymax': 310}\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<img src=\"https://currumbinvetservices.com.au/wp-content/uploads/2015/09/indian-ringneck.jpg\" width=\"500\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detections: parrot (score:0.598288893699646\n",
      "{'xmin': 5, 'ymin': 117, 'xmax': 705, 'ymax': 574}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "import pandas as pd\n",
    "from IPython.display import Image, display, Markdown\n",
    "\n",
    "detector = pipeline(model=\"google/owlvit-base-patch32\", task=\"zero-shot-object-detection\")\n",
    "\n",
    "examples = [(\"https://farm4.staticflickr.com/3266/3247626615_b3ab8a85af_z.jpg\",[\"person\",\"woman\",\"man\"]),\n",
    "           (\"https://currumbinvetservices.com.au/wp-content/uploads/2015/09/indian-ringneck.jpg\",[\"parrot\",\"bird\",'flamingo'])]\n",
    "\n",
    "\n",
    "\n",
    "for (image,candidate_labels) in examples:\n",
    "    \n",
    "    response = detector(image=image, candidate_labels=candidate_labels)\n",
    "    \n",
    "    display(Image(url= image,width=500))\n",
    "    print(f\"Detections: {response[0]['label']} (score:{response[0]['score']}\")\n",
    "    print(response[0]['box'])\n",
    "    print()\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-modal tasks:\n",
    "\n",
    "### Automatic Captioning:\n",
    "\n",
    "Image To Text pipeline is designed for AutoModelForVision2Seq. The pipeline predicts a caption for a given image. It can be used with [other models](https://huggingface.co/models?pipeline_tag=image-to-text)\n",
    "\n",
    "#### Inferring from [**ydshieh/vit-gpt2-coco-en**](https://huggingface.co/ydshieh/vit-gpt2-coco-en)\n",
    "It's not a state of the art model, but it works in a reasonable way for simple images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://farm4.staticflickr.com/3607/3567935365_dc4880fa10_z.jpg\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Predicted caption:\n",
       "<text style=color:blue>a car is stopped at a red light </text>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "from IPython.display import Image, display\n",
    "\n",
    "\n",
    "image_url = \"https://farm4.staticflickr.com/3607/3567935365_dc4880fa10_z.jpg\"\n",
    "\n",
    "\n",
    "captioner = pipeline(model=\"ydshieh/vit-gpt2-coco-en\", max_new_tokens=50)\n",
    "display(Image(url= image_url))\n",
    "\n",
    "caption = captioner(image_url)[0]['generated_text']\n",
    "\n",
    "HTML((f'Predicted caption:\\n{colored(caption)}'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Document Question Answering:\n",
    "\n",
    "Document Question Answering pipeline can be used with any AutoModelForDocumentQuestionAnswering. \n",
    "\n",
    "The pipeline takes an image (and optional OCR’d words/boxes) as input and it generates answers according to its content.\n",
    "\n",
    "The pipeline can be used with [other models](https://huggingface.co/models?pipeline_tag=document-question-answering)\n",
    "\n",
    "#### Inferring from [**impira/layoutlm-document-qa**](https://huggingface.co/impira/layoutlm-document-qa)\n",
    "\n",
    "The model has been fine-tuned on SQuAD2.0 and DocVQA.\n",
    "\n",
    "IMPORTANT: In addition to transformers, it requires also **PIL**, **pytesseract**, and **PyTorch**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://ec.europa.eu/eurostat/documents/4187653/13722720/Gender_pay_gap_2020.png/\" width=\"500\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What is the difference average gross hourly earnings between males and females in Europe? \n",
      "Answer:13.0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<img src=\"https://ec.europa.eu/eurostat/documents/4187653/13722720/Gender_pay_gap_2020.png/\" width=\"500\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What is the difference in pay gap between Italy and Poland? \n",
      "Answer:4.2\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "import pandas as pd\n",
    "from IPython.display import Image, display\n",
    "\n",
    "\n",
    "questions = ['What is the difference average gross hourly earnings between males and females in Europe?',\n",
    "             'What is the difference in pay gap between Italy and Poland?']\n",
    "images = ['https://ec.europa.eu/eurostat/documents/4187653/13722720/Gender_pay_gap_2020.png/',\n",
    "          'https://ec.europa.eu/eurostat/documents/4187653/13722720/Gender_pay_gap_2020.png/']\n",
    "\n",
    "document_qa = pipeline(model=\"impira/layoutlm-document-qa\")\n",
    "\n",
    "\n",
    "for question, image in zip(questions,images):\n",
    "    result = document_qa(image = image, question=question)\n",
    "    answer = result[0]['answer']\n",
    "    display(Image(url= image,width=500))\n",
    "    print(f'Question: {question} \\nAnswer:{answer}')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visual Question Answering:\n",
    "\n",
    "The Visual Question Answering pipeline can be used with any AutoModelForVisualQuestionAnswering. \n",
    "\n",
    "The pipeline takes an image (and optional OCR’d words/boxes) as input and it generates answers according to its content.\n",
    "\n",
    "The pipeline can be used with [other models](https://huggingface.co/models?pipeline_tag=visual-question-answering)\n",
    "\n",
    "#### Inferring from [**vilt-b32-finetuned-vqa**](https://huggingface.co/dandelin/vilt-b32-finetuned-vqa)\n",
    "\n",
    "Vision-and-Language Transformer (ViLT) model fine-tuned on VQAv2.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://huggingface.co/datasets/Narsil/image_dummy/raw/main/lena.png\" width=\"500\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Is this a man? \n",
      "Answer:no\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<img src=\"http://images.cocodataset.org/val2017/000000039769.jpg\" width=\"500\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What is the subject of the photo? \n",
      "Answer:cat\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<img src=\"https://farm5.staticflickr.com/4046/4314731899_4baf64470f_z.jpg\" width=\"500\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: How many people are in the picture? \n",
      "Answer:4\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "vqa = pipeline(model=\"dandelin/vilt-b32-finetuned-vqa\")\n",
    "image_url = \"https://farm5.staticflickr.com/4046/4314731899_4baf64470f_z.jpg\"\n",
    "\n",
    "\n",
    "questions = ['Is this a man?',\n",
    "             'What is the subject of the photo?',\n",
    "             'How many people are in the picture?']\n",
    "images = ['https://huggingface.co/datasets/Narsil/image_dummy/raw/main/lena.png',\n",
    "          'http://images.cocodataset.org/val2017/000000039769.jpg',\n",
    "          'https://farm5.staticflickr.com/4046/4314731899_4baf64470f_z.jpg']\n",
    "\n",
    "for question, image in zip(questions,images):\n",
    "    result = vqa(image = image, question=question)\n",
    "    answer = result[0]['answer']\n",
    "    display(Image(url= image,width=500))\n",
    "    print(f'Question: {question} \\nAnswer:{answer}')\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lectures_lab",
   "language": "python",
   "name": "lectures_lab"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
